{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
   "metadata": {},
   "source": [
    "## Agent Supervisor\n",
    "\n",
    "# <img src=\"./img/supervisor-diagram.png\" width=\"500\">\n",
    "\n",
    "To simplify the code in each agent node, we will use the AgentExecutor class from LangChain. This and other \"advanced agent\" notebooks are designed to show how you can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac25624-4d83-45a4-b9ef-a10589aacfb7",
   "metadata": {},
   "source": [
    "## Create tools\n",
    "\n",
    "For this example, you will make an agent to do web research with a search engine, and one agent to create plots. Define the tools they'll use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04c6778-403b-4b49-9b93-678e910d5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# This executes code locally, which can be unsafe\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5efcf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "\n",
    "working_directory = 'tmp'\n",
    "\n",
    "toolkit = FileManagementToolkit(\n",
    "    root_dir=str(working_directory)\n",
    ")  # If you don't provide a root_dir, operations will default to the current working directory\n",
    "toolkit.get_tools()\n",
    "\n",
    "file_tools = FileManagementToolkit(\n",
    "    root_dir=str(working_directory),\n",
    "    selected_tools=[\"read_file\", \"write_file\", \"list_directory\"],\n",
    ").get_tools()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d1e85-22d4-4c22-9062-72a346a0d709",
   "metadata": {},
   "source": [
    "## Helper Utilities\n",
    "\n",
    "Define a helper function below, which make it easier to add new agent worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4823dd9-26bd-4e1a-8117-b97b2860211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c302b0-cd57-4913-986f-5dc7d6d77386",
   "metadata": {},
   "source": [
    "We can also define a function that we will use to be the nodes in the graph - it takes care of converting the agent response to a human message. This is important because that is how we will add it the global state of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80862241-a1a7-4726-bce5-f867b233832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32962d2-5487-496d-aefc-2a3b0d194985",
   "metadata": {},
   "source": [
    "### Create Agent Supervisor\n",
    "\n",
    "It will use function calling to choose the next worker node OR finish processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311f0a58-b425-4496-adac-dc4cd8ffb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "members = [\"Researcher\", \"Coder\", \"FileAgent\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d507f-34d1-4f1b-8dde-5e58d17b2166",
   "metadata": {},
   "source": [
    "## Construct Graph\n",
    "\n",
    "We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08577be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a430af7-8fce-4e66-ba9e-d940c1bc48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n",
    ")\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "file_agent = create_agent(\n",
    "    llm,\n",
    "    file_tools,\n",
    "    \"You can list dir, read and write files.\",\n",
    ")\n",
    "file_node = functools.partial(agent_node, agent=file_agent, name=\"FileAgent\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", code_node)\n",
    "workflow.add_node(\"FileAgent\", file_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1593d5-39f7-4819-96d2-4ad7d7991d72",
   "metadata": {},
   "source": [
    "Now connect all the edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14778e86-077b-4e6a-893c-400e59b0cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36496de-7121-4c49-8cb6-58c943c66628",
   "metadata": {},
   "source": [
    "## Invoke the team\n",
    "\n",
    "With the graph created, we can now invoke it and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56ba78e9-d9c1-457c-a073-d606d5d3e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n",
      "{'Coder': {'messages': [HumanMessage(content='Hello, World!', name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n",
      "{'Coder': {'messages': [HumanMessage(content='Here is the \"Hello, World!\" message printed to the terminal:\\n\\n```\\nHello, World!\\n```', name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FileAgent'}}\n",
      "----\n",
      "{'FileAgent': {'messages': [HumanMessage(content=\"The `hello_world.py` file has been created successfully with the following content:\\n\\n```python\\nprint('Hello, World!')\\n```\\n\\nNext, you can run this script in the terminal to see the output. If you need help running the script, let me know!\", name='FileAgent')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45a92dfd-0e11-47f5-aad4-b68d24990e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Researcher'}}\n",
      "----\n",
      "{'Researcher': {'messages': [HumanMessage(content='The latest OpenAI GPT model is GPT-4 Turbo. This model supports both text and vision inputs, and it has enhanced capabilities compared to previous versions. You can find more information about the GPT-4 Turbo model on the [OpenAI platform documentation](https://platform.openai.com/docs/models).\\n\\nHere\\'s a sample Python API call to use the GPT-4 Turbo model:\\n\\n1. **Install OpenAI Python library**:\\n    ```bash\\n    pip install openai\\n    ```\\n\\n2. **Write the Python code**:\\n    ```python\\n    import openai\\n\\n    # Set your OpenAI API key here\\n    openai.api_key = \\'your-api-key\\'\\n\\n    # Make a request to the GPT-4 Turbo model\\n    response = openai.ChatCompletion.create(\\n        model=\"gpt-4-turbo\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\\n        ]\\n    )\\n\\n    # Print the response\\n    print(response.choices[0].message[\\'content\\'])\\n    ```\\n\\n3. **Run your script**:\\n    ```bash\\n    python your_script_name.py\\n    ```\\n\\nReplace `\\'your-api-key\\'` with your actual OpenAI API key. This script sends a message to the GPT-4 Turbo model and prints out the response.', name='Researcher')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n",
      "{'Coder': {'messages': [HumanMessage(content='Here\\'s a more detailed explanation of each part of the Python script:\\n\\n1. **Installation**:\\n    Make sure you have the OpenAI Python library installed. You can install it using pip:\\n    ```bash\\n    pip install openai\\n    ```\\n\\n2. **Importing the OpenAI Library**:\\n    In your Python script, start by importing the OpenAI library.\\n    ```python\\n    import openai\\n    ```\\n\\n3. **Setting the API Key**:\\n    You need to set your OpenAI API key to authenticate your requests. Replace `\\'your-api-key\\'` with your actual API key from OpenAI.\\n    ```python\\n    openai.api_key = \\'your-api-key\\'\\n    ```\\n\\n4. **Making a Request to the GPT-4 Turbo Model**:\\n    You can create a completion request to the GPT-4 Turbo model using the `ChatCompletion.create` method. This example shows how to send a message to the model and get a response.\\n    ```python\\n    response = openai.ChatCompletion.create(\\n        model=\"gpt-4-turbo\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\\n        ]\\n    )\\n    ```\\n\\n    - `model`: Specifies the model to use. Here, it\\'s set to `\"gpt-4-turbo\"`.\\n    - `messages`: A list of messages to send to the model. The first message is from the system, setting the context, and the second message is from the user.\\n\\n5. **Printing the Response**:\\n    The response from the model is stored in the `response` variable. You can access the content of the response as follows:\\n    ```python\\n    print(response.choices[0].message[\\'content\\'])\\n    ```\\n\\n6. **Running the Script**:\\n    Save your script and run it using Python. For example:\\n    ```bash\\n    python your_script_name.py\\n    ```\\n\\nBy following these steps, you can interact with the latest GPT-4 Turbo model from OpenAI and obtain responses to your queries. If you have any specific requirements or additional questions, feel free to ask!', name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FileAgent'}}\n",
      "----\n",
      "{'FileAgent': {'messages': [HumanMessage(content=\"I have created a Python script named `gpt4_turbo_api_call.py` with the sample API call to the GPT-4 Turbo model. You can find the file in your directory.\\n\\nTo run the script, make sure you have the OpenAI Python library installed and replace `'your-api-key'` with your actual OpenAI API key. Then, you can execute the script using the following command:\\n\\n```bash\\npython gpt4_turbo_api_call.py\\n```\\n\\nThis script will send a message to the GPT-4 Turbo model and print out the response. If you need any further assistance, feel free to ask!\", name='FileAgent')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Can you look for the latest openai gpt model and write a sample api calling in python.\")]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d363d2c-e0da-4cce-ba47-ad2aa9df0fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
